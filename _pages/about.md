---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

My name is Xiyang Wu. I am a Ph.D. student in Electrical and Computer Engineering at [University of Maryland, College Park](https://umd.edu/) and a member of [GAMMA](https://gamma.umd.edu/) group. My research advisor is Prof. [Dinesh Manocha](https://www.cs.umd.edu/people/dmanocha). I hold a Master's degree from [Georgia Institute of Technology](https://www.gatech.edu/), where I was working with Prof. [Matthew Gombolay](https://core-robotics.gatech.edu/people/matthew-gombolay/). Before that, I earned my Bachelor's in Engineering from [Tianjin University](https://www.tju.edu.cn/english/index.htm), supervised by Prof. [Xiaodong Zhang](https://scholar.google.com/citations?user=as6X3L0AAAAJ&hl=en).

My research interest lies in using the reasoning ability of neural networks and Large Language Models in robot decision-making and navigation tasks. Specifically, incorporating Large Language Models with reinforcement learning approaches in multi-agent coordination, scene understanding, and opponents' intention inference when navigating and collaborating under complex and heterogeneous environments.

Please check list of publications [here](http://wuxiyang1996.github.io/publications/).

Research Interest
======
- Robotics
- Reinforcement Learning
- Multi-Modality
- Vision Language Model

Education
======
 - Ph.D. in Electrical and Computer Engineering, University of Maryland, College Park, 2021 - 2026 (Expected)
 - M.S. in Electrical and Computer Engineering, Georgia Institute of Technology, 2019 - 2021
 - B.Eng. in Electrical Engineering (Honors Class), Tianjin University, 2015 - 2019

News
======

<style>
/* table {
    border-collapse: collapse!important;
    font-size: 18px!important;
    border: none!important;
} */
td, th {
    border: none!important;
    padding-top: 0px;
    padding-bottom: 0px;
  /* padding-left: 30px;
  padding-right: 40px; */
}

</style>
<div style="height:300px;overflow:auto;">
<table style="border-collapse: collapse;font-size: 18px;border: none;">
<col width="110px">
<!-- <col width="630px"> -->
  <!-- <tr><td><b>Timeline</b></td><td><b>Updates</b></td></tr> -->
  <tr><td><b>Jun 2025:</b></td><td> <a style="color:black" href="/adversary_robot">One paper</a> was accepted <b>IROS 2025</b> </td></tr>
  <tr><td><b>May 2025:</b></td><td> We release a <a style="color:black" href="https://arxiv.org/abs/2505.01481">technical report</a>, introducing a novel benchmark for hallucinations in synthetic video understanding over common sense and physics, <a style="color:black" href="/videohallu">VideoHallu</a>, with QA pairs requiring human-level reasoning. The goal of this benchmark is to evaluate and post-train SoTA MLLMs on commonsense/physics data shows its impact on improving model reasoning. The project webpage is released <a style="color:black" href="https://wuxiyang1996.github.io/videohallu_page/">here</a>. </td></tr>
  <tr><td><b>Sep 2024:</b></td><td><a style="color:black" href="/autohallusion">AUTOHALLUSION</a> was accepted by <b>EMNLP 2024</b>!</td></tr>
  <tr><td><b>Jun 2024:</b></td><td><a style="color:black" href="/lancar">LANCAR</a> and <a style="color:black" href="/agl_net">AGL-NET</a> were accepted by <b>IROS 2024</b>!</td></tr>
  <tr><td><b>Jun 2024:</b></td><td> We release a <a style="color:black" href="https://arxiv.org/abs/2406.10900">technical report</a>, introducing a novel automatic benchmark generation approach, <a style="color:black" href="/autohallusion">AUTOHALLUSION</a>, which harnesses a few principal strategies to create diverse hallucination examples by probing the language modules in LVLMs for context cues. The project webpage is released <a style="color:black" href="https://wuxiyang1996.github.io/autohallusion_page/">here</a>. </td></tr>
  <tr><td><b>Apr 2024:</b></td><td> <a style="color:black" href="/adversary_robot">One paper</a> was accepted by <b>VLADR Workshop</b> at <b>CVPR 2024</b>!</td></tr>
  <tr><td><b>Feb 2024:</b></td><td><a style="color:black" href="/hallusionbench">HallusionBench</a> was accepted by <b>CVPR 2024</b>! The data, evaluation and code are available on <a style="color:black" href="https://github.com/tianyi-lab/HallusionBench">GitHub</a>.</td></tr>
  <tr><td><b>Feb 2024:</b></td><td> We release a <a style="color:black" href="https://arxiv.org/abs/2402.10340">technical report</a> highlighting the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. The project webpage is released <a style="color:black" href="https://wuxiyang1996.github.io/adversary-vlm-robotics/">here</a>. </td></tr>
  <tr><td><b>Oct 2023:</b></td><td> We release an <a style="color:black" href="https://huggingface.co/papers/2310.14566">early report</a> and analysis on failure modes of GPT-4V and LLaVA-1.5. Stay tuned on the release of our dataset <a style="color:black" href="/hallusionbench">HallusionBench</a>!</td></tr>
  <tr><td><b>Oct 2023:</b></td><td><a style="color:black" href="/iplan">iPLAN</a> was award as <strong style="color:red">Best Paper Award</strong> by <b>MRS Workshop</b> at <b>IROS 2023</b>!</td></tr>
  <tr><td><b>Aug 2023:</b></td><td><a style="color:black" href="/iplan">iPLAN</a> was accepted by <b>CoRL 2023</b> with <strong style="color:red">Oral Presentation (Accept Rate: 6.6%)</strong> !</td></tr>
  <tr><td><b>Jul 2023:</b></td><td> <a style="color:black" href="/photometric">One paper</a> was accepted by <b>Digital Signal Processing</b>!</td></tr>
  <tr><td><b>Aug 2021:</b></td><td>Started Ph.D. at University of Maryland, College Park.</td></tr>
  <tr><td><b>Aug 2019:</b></td><td>Started M.S. at Georgia Institute of Technology.</td></tr>
</table>
</div>


<br>


Selected Publications<a id="pub"></a>
======


{% include base_path %}

{% include archive-compact.html %}