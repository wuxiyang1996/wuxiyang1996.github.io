---
title: "AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales"
collection: publications
permalink: /publications/AGL_NET_03_15_2024
excerpt: "We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment."
date: 2024-03-15
venue: 'The IEEE/RSJ International Conference on Intelligent Robots and Systems'
short: 'IROS 2024'
paperurl: 'https://arxiv.org/abs/2404.03187'
teaser: '../images/agl_net.png'
authors: "Tianrui Guan*, Ruiqi Xian*, Xijun Wang, <b>Xiyang Wu</b>, Mohamed Elnoor, Daeun Song, Dinesh Manocha (* indicates equal contributions)"
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
redirect_from: 
  - /agl_net
---

<p style="text-align:center;">
<img src="../images/agl_net.png" width="800">
</p>

## Abstract
<div style="text-align: justify"> We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment.</div>
<br>

| Paper                                         |  
|-----------------------------------------------|
| [**LANCAR**](https://arxiv.org/abs/2404.03187) |
<br>

Please cite our work if you found it useful,

```
@article{guan2024agl,
  title={AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales},
  author={Guan, Tianrui and Xian, Ruiqi and Wang, Xijun and Wu, Xiyang and Elnoor, Mohamed and Song, Daeun and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2404.03187},
  year={2024}
}
```