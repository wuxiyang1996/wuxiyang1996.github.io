---
title: "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models"
collection: publications
permalink: /publications/MASS_11-23_2025
excerpt: "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning 
involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or 
AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that 
addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' 
perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 
real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension 
tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion 
tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the 
VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. 
To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show 
that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% 
and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and 
comprehension. These results validate the effectiveness of our approach."
date: 2025-11-23
venue: 'arXiv:2511.18373'
short: 'arXiv'
paperurl: 'https://www.arxiv.org/abs/2511.18373'
teaser: '/images/mass.png'
authors: "<b>Xiyang Wu*</b>, Zongxia Li, Jihui Jin, Guangyao Shi, Gouthaman KV, Vishnu Raj, Nilotpal Sinha, Jingxi Chen, Fan Du, Dinesh Manocha"
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
project: ""
code: ""
dataset: ""
redirect_from: 
  - /mass
---

<p style="text-align:center;">
<img src="/images/VideoHallu.png" width="800">
</p>

## Abstract
<div style="text-align: justify"> Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning 
involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or 
AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that 
addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' 
perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 
real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension 
tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion 
tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the 
VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. 
To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show 
that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% 
and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and 
comprehension. These results validate the effectiveness of our approach.
</div>
<br>


| Paper                                            | Project Website                                                  | Code                                                        | Dataset        | 
|--------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------------|----------------|
| [**MASS**](https://www.arxiv.org/abs/2511.18373) | To Be Released | To Be Released | To Be Released |

<br>

Please cite our work if you found it useful,

```
@misc{wu2025massmotionawarespatialtemporalgrounding,
      title={MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models}, 
      author={Xiyang Wu and Zongxia Li and Jihui Jin and Guangyao Shi and Gouthaman KV and Vishnu Raj and Nilotpal Sinha and Jingxi Chen and Fan Du and Dinesh Manocha},
      year={2025},
      eprint={2511.18373},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.18373}, 
}
```